{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9f1c30-df67-422d-aa58-55f11fd718f0",
   "metadata": {},
   "source": [
    "### Function for getting the Food101 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2418fea9-5b2c-4cdc-af39-e542459e3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Food101\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_data(data_path, transform):\n",
    "    food101_train_data = Food101(\n",
    "        root = data_path / \"train\",\n",
    "        split = 'train',\n",
    "        transform = transform,\n",
    "        target_transform = None,\n",
    "        download = True\n",
    "    )\n",
    "\n",
    "    classes = food101_train_data.classes\n",
    "\n",
    "    class_count = {}\n",
    "    indices = []\n",
    "    \n",
    "    for i, (_, label) in enumerate(food101_train_data):\n",
    "        if label not in class_count.keys():\n",
    "            class_count[label] = 0\n",
    "\n",
    "        if class_count[label] < 160:\n",
    "            class_count[label] += 1\n",
    "            indices.append(i)\n",
    "\n",
    "\n",
    "    train_data = Subset(\n",
    "        dataset = food101_train_data, \n",
    "        indices = indices\n",
    "    )\n",
    "\n",
    "\n",
    "    food101_test_data = Food101(\n",
    "        root = data_path / \"test\",\n",
    "        split = 'test',\n",
    "        transform = transform,\n",
    "        target_transform = None,\n",
    "        download = True\n",
    "    )\n",
    "\n",
    "    class_count = {}\n",
    "    indices = []\n",
    "\n",
    "    for i, (_, label) in enumerate(food101_test_data):\n",
    "        if label not in class_count.keys():\n",
    "            class_count[label] = 0\n",
    "\n",
    "        if class_count[label] < 40:\n",
    "            class_count[label] += 1\n",
    "            indices.append(i)\n",
    "\n",
    "    test_data = Subset(\n",
    "        dataset = food101_test_data,\n",
    "        indices = indices\n",
    "    )\n",
    "        \n",
    "    return train_data, test_data, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8d291-b7cd-4fa3-bf31-82cbf8e078e6",
   "metadata": {},
   "source": [
    "### Function to create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db960a2-0a24-4f64-a013-2660e57df599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloaders(train_data, test_data):\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset = train_data,\n",
    "        batch_size = 32,\n",
    "        num_workers = 1,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset = test_data,\n",
    "        batch_size = 32,\n",
    "        num_workers = 1,\n",
    "        shuffle = False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3d223-91eb-4e04-985a-f8c06213b1a4",
   "metadata": {},
   "source": [
    "### Function to create VisionTransformer (ViT) feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13426191-9bf9-4db8-947d-01215b69b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ViT_B_16_Weights, vit_b_16\n",
    "from torch import nn\n",
    "\n",
    "def get_model(out_features):\n",
    "    model = vit_b_16(\n",
    "        weights = ViT_B_16_Weights.DEFAULT\n",
    "    )\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.heads.head = nn.Linear(\n",
    "        in_features = 768, \n",
    "        out_features = out_features, \n",
    "        bias = True\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a381c-6e8a-432d-ae24-8602ed407487",
   "metadata": {},
   "source": [
    "### Function to create image transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b85ba2-343b-459f-994c-cd041bcffec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ViT_B_16_Weights\n",
    "\n",
    "def get_transform():\n",
    "    return ViT_B_16_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6d580dc-53b6-4eda-8745-d7534dbe7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('../data')\n",
    "\n",
    "transform = get_transform()\n",
    "train_data, test_data, classes = get_data(data_path, transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89f006-b034-4785-87da-5fcdf1314c67",
   "metadata": {},
   "source": [
    "### Function for train engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e84cca5-00da-4ffc-80ca-1c249d8a90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "def train_set(model, dataloader, loss_fn, optimizer, device):\n",
    "\n",
    "    train_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for X, y in dataloader:\n",
    "\n",
    "        X.to(device)\n",
    "        y.to(device)\n",
    "        \n",
    "        y_logits = model(X)\n",
    "        loss = loss_fn(y_logits, y)\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        accuracy += accuracy_score(y.detach().numpy(), y_pred.detach().numpy())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    accuracy = accuracy / len(dataloader) * 100\n",
    "\n",
    "    return train_loss, accuracy\n",
    "\n",
    "\n",
    "def test_set(model, dataloader, loss_fn, device):\n",
    "\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            X.to(device)\n",
    "            y.to(device)\n",
    "\n",
    "            y_logits = model(X)\n",
    "            loss = loss_fn(y_logits, y)\n",
    "            y_pred = torch.softmax(y_logits, dim=1).argamx(dim=1)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            accuracy += accuracy_score(y.detach().numpy(), y_pred.detach().numpy())\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    accuracy = accuracy / len(dataloader) * 100\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6750cb-cfa7-4af4-a201-129611c22d84",
   "metadata": {},
   "source": [
    "### Function for engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c134c406-5965-42d9-bc44-29f2d95251e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engine(model, train_dataloader, test_dataloader, loss_fn, optimizer, device, epochs):\n",
    "\n",
    "    result = {\n",
    "        'epoch' : [],\n",
    "        'train_loss' : [],\n",
    "        'train_acc' : [],\n",
    "        'test_loss' : [],\n",
    "        'test_acc' : []\n",
    "    }\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        train_loss, train_acc = train_set(\n",
    "            model = model, \n",
    "            dataloader = train_dataloader, \n",
    "            loss_fn = loss_fn, \n",
    "            optimizer = optimizer, \n",
    "            device = device\n",
    "        )\n",
    "\n",
    "        test_loss, test_acc = test_set(\n",
    "            model = model,\n",
    "            dataloader = test_dataloader,\n",
    "            loss_fn = loss_fn,\n",
    "            device = device\n",
    "        )\n",
    "\n",
    "        result['epoch'].append(i+1)\n",
    "        result['train_loss'].append(train_loss)\n",
    "        result['train_acc'].append(train_acc)\n",
    "        result['test_loss'].append(test_loss)\n",
    "        result['test_acc'].append(test_acc)\n",
    "\n",
    "        print(f\"epoch = {epoch} | train loss = {train_loss}  | train acc = {train_acc} | test loss = {test_loss} | test acc = {test_acc}\")\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde61bc-c6b2-4595-9c2f-9b0ecac6f18b",
   "metadata": {},
   "source": [
    "### Function for save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6df0258f-0354-465e-a93f-b96ec69895c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, model):\n",
    "    file_name = path / \"vit_model_on_food101.pth\"\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    print(f\"The model has been saved successfully in {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d350c5a-8a0a-4207-b79f-293639be64d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
